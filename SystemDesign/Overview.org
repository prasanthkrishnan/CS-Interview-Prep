#+TITLE: System design overview of a typical website
#+AUTHOR: Prasanth Krishnan
#+EMAIL: knp281192@gmail.com
#+DESCRIPTION: This file contains my notes from various sources, mainly system-design-primer.
#+OPTIONS: toc:2

* VSPs - EC2 etc

* Vertical scaling vs Horizontal scaling.
** vertical scaling
Throwing moneyu to get bigger machines

** Horizontal scaling
- Adding more cheaper machines
- It was easier to have website hosted a single machine, but with vertical scaling we have multiple machines. How to let customer know about all these group of machines ? - DNS.
- We also want to distribute the requests in a balanced way to all the web hosts - Put a Load balancer before it.

* DNS
+ DNS - Domain name server, maps domain names to ip addresses.
+ If there are multiple back end hosts then, they will be behind a load balancer.
+ If there is a Load balancer then DNS should resolve to LB hosts.

* Load Balancer 
+ Load balancer is a set of hosts that sits in between the client and the backend web hosts.
+ LB provides abstraction that there are multiple backend hosts from the client.
+ It handles and http request and then routes them according to the routing policy.
+ commonly used routing policies: Roud robin, least connections etc.
+ To route requests, LB has to know which backend hosts are alive -- health checks.
+ In complex routing policies, it also needs to query the backend hosts for data. i.e. policy - send request to the least busy server. LB has to know the load(CPU used) on each machine.
+ With LB, back end hosts can be assigned only with private ip(remain in a local network), thus enhancing the security.
+ Other uses of LB is to route traffic based on type of request to different set of hosts(different service). i.e. different services handling different URLs.

** why not use DNS to load balance among backend hosts?
Lets say, we have a DNS based LB solution, which is gaurenteed to return the ip address of the backend hosts in a round robin fashion. This solution works, client taks to DNS server, it gives back a backend ip address and client contacts the backend host to get the content. But there are problems with this approach.
+ DNS servers doesnt talk to the backend hosts so they cant do complicated routing polcies.
+ Its not easy to update DNS when a backend hosts fails or when adding new hosts.
+ Operating system also cache the DNS query and its result, that means second requests from the same client maching will not do a DNS query but reuse the result from previous query, thus failing to load balancing.

** How to maintain session state, like shopping cart / user login ?
Prior to LB, all the state can be stored in the single machine's HDD. But with load balancing, subsequent request might go to different server, there by cant use reuse session state. How about storing the session state outside the servers in a Database (duh!). But the next question is how do we prevent, the DB from crashing? Will look at it in replication.

** Can we use cokkies to store the address of the backend server that has the stored session state ?
If we store the address of the backend server in cookies and let LB use that info to route the request we could technically use cookies to route traffic to the backend server with the session informaion. But cookies can be user controlled, thus a harmful user can exploit it to DDOS the backend server. User could disable cookie in their browser.

** How are load balancers implemented ?
*** Software LB
+ AWS ELB / ALB / NLB
+ HAProxy
+ Ngnix
*** Hardware LB
+ Cisco
+ Citrix

* Databases
Once the application servers can horizontally scale (with the help of a DB storing the state), the next bottle neck will be the database.
- Having only one instance as your DB poses a availabity and scalability problem.
- Availabity problem can be solved by having (read) replicas to the master. This doesnt solve the scalability problem especially for writes.
** DB Sharding
Scalabity can be mitigated somewhat by sharding the DB into multiple instances. But this requires careful thought every time it has to be sharded.
** Moving to NoSQL
NoSQL stores data in a denormalized format. The joins are done in the application code rather than the DB. NoSQL DB like DDB, mongoDB scale very well because of this and looser consistency gaureentees.

* Caching
** Caching DB queries
Pros - DO the heavy compution like join once and store it the cache and reuse it.
Cons :
- If the data changes then the cached query result is invalid and have to recompute.
- There might more combinations of queries ur application supports than the objects themselves.
** Caching Data objects
Pros 
- No need to store every combination of the query.
- Leverage asychronous refreshing by a army of workers who assemble the objects stored in the cache from database.
Cons - Query have to be re computed.
** Can Precompute all pages and store them as static htmls and just vend them upon request?
yes we can, in this approach, backend servers are just file vendors. Backend servers just respond with the static html page mathcing the request. But there are downsides to this:
+ This requires creating a lot of static pages and each of them have the same repeated html content but only the values changes.
+ If we have to change the design of the page, we have modify all the static pages.

The typical approach is to have templates(view) and let backend servers write the dynamic data(model) upon the template
** Distributed caches Memcached / Redis

* Asynchornism
** Precomputing
If there are heavy work that needs to be done, instead of starting one on demand, pre compute and store the result. Like convert dynamic pages into static ones as possible and store them. Return the precomputed stuff on demand. This makes the request time very less and makes the website very snappy. The disadvantage is that content may change and that means we have compute the result again. Have a periodic cron job whihc does this.
** Processing heavy stuff in the background and letting customer know when its done.
when a customer wants to do a heavy computation, which cannot be pre computed we do it asychronously. Accept the request put it into a queue like SQS or rabbitMQ and send a confirmation to the customer tellking them it is in progress. Have a bunhc of workers which process the queue and publish the result. Cusotmer can be notified when it done. This can be in a status web page which peridically polls if the job is done or not or via notifications like SNS.

* High level Tradeoffs
** Performance vs Scalability
A service is scalable if adding more resources ==> increases performance. Some times the service needs to be alwasy-on, so to make it reliable redudancy is introduced. A service is also considered scalable if its performance doesnt degrade when more resources are introduced for redudancy.
** Latency vs throughput
Latency is the time to perform some action or to produce some result. (ms)
Throughput is the number of such actions or results per unit of time. (tps)
Generally, you should aim for maximal throughput with acceptable latency. (y tps that can achieved with P99 latency x)

** Availabity vs Consistency
From CAP theorem there can only be two gaurentees supported by a distributed system. 
- CP
- AP
- http://ksat.me/a-plain-english-introduction-to-cap-theorem/

*** Consistency patterns
**** Replicated Data consistency explained through Baseball
[[http://pages.cs.wisc.edu/~remzi/Classes/739/Fall2018/Papers/baseball-13.pdf][Paper]]
- *Strong Consistency* - All reads return the latest value of an object following a write.
- *Eventually consistency* - Reads return stale data, i.e. The  data  returned  by a read operation is the value of the object  at  some  past  point  in  time  but not  necessarily  the  latest  value. This is done in favour of better performance and availability. This is usually implemented by asychronous replication.
- Recent systems, recognizing the need to support different classes of applications, have been designed with a choice of operations for accessing cloud storage.   
***** Flavors of Read Consistency Gaureentees. 
| Strong Consistency   | See all previous writes.                            |
| Eventual Consistency | See subset of previous writes.                      |
| Consistent Prefix    | See initial sequence of writes.                     |
| Bounded Staleness    | See all “old” writes.                               |
| Monotonic reads      | See increasing subset of writes over multiple reads |
| Read My Writes       | See all writes performed by reader                  |

***** Read Gaurentees for various clients
| Clients              | Desc                                             | Read Gaurentees                     |
|----------------------+--------------------------------------------------+-------------------------------------|
| Official scorekeeper | a single person updating the score for every run | Read My Writes                      |
| Umpire               | Needs the score only before the last innings.    | Strong Consistency                  |
| Radio reporter       | peridically announces scores during the game     | Consistent Prefix & Monotonic reads |
| Sports writer        | Writes articles for the morning news paper       | Bounded Staleness                   |
| Statistician         | Writes stats about the team after the game       | Strong Consistency, read My Writes  |
| Stat watcher         | Person occasionally visiting teams old stats     | Eventual Consistency                |

***** Conclusion
- All of the six presented consistency guarantees are useful. 
- Different clients may want different consistencies even when accessing the same data.
  - Thus a particular type of data is not associated with a consistency gaurentee. i.e. banking info with strong consistency.
- Clients  should  be  able  to  choose  their desired consistency.

*** Availabity patterns
**** Fail over
***** Active passive / master slave
With active-passive fail-over, heartbeats are sent between active and passive server on standby. If the heartbeat is interrupted, the passive server takes over the active's IP address and resumes service. The length of downtime is determined by whether the passive server is already running in 'hot' standby or whether it needs to start up from 'cold' standby. Only the active server handles traffic.
***** Active Active / master master 
In active-active, both servers are managing traffic, spreading the load between them.
**** Replication
Discussed in DB section
**** Availabity definition
Availability is often quantified by uptime (or downtime) as a percentage of time the service is available. 2-9s=99.0 3-9s=99.9 4-9s=99.99

