#+TITLE: Dynamic Programming
#+AUTHOR: Prasanth Krishnan
#+EMAIL: knp281192@gmail.com
#+DESCRIPTION: This file contains my notes and coding samples for the topic Dynamic Programming.
#+OPTIONS: toc:2

* What is Dynamic Programming ?
Dynamic programming is a paradigm which gives us a way to create custom solutions to optimization problems. Optimization problem usually involve minimzing or maximizing a cost function. Algorithms for optimization problems require proof that they always return the best possible solution. Greedy algorithms that make the best local decision at each step are typically efficient but usually do not guarantee global optimality. Exhaustive search algorithms like backtracking, that try all possibilities and select the best always produce the optimum result, but usually at a prohibitive cost in terms of time complexity.

Dynamic programming combines the best of both worlds. It gives us a way to design custom algorithms that systematically search all possibilities (thus guaranteeing correctness) while storing results to avoid recomputing (thus providing efficiency). By storing the consequences of all possible decisions and using this information in a systematic way, the total amount of work is minimized

** Fibonacci series
DP is technique for efficiently implementing a recursive algorithm by storing the partial results and reusing them instead of computing them. Over used example is Fibonacci numbers.

#+begin_src java
f(n) = f(n - 1) + f (n - 2)
#+end_src

A recursive version uncached version. It takes exponential runtime, simple because same fib(i) value is recomputed several times.
#+begin_src java
private int fib(int n) {
    if(n == 0) return 0;
    if(n == 1) return 1;

    return fib(n - 1) + fib(n - 2);
}
#+end_src

A recursive cached version. This takes linear time ~O(n)~. Since each fib(i) is calculated exactly once and reused after.
#+begin_src java
class Solution {
    int UNKNOWN = -1;
    int[] f;

    private int fib(int n) {
        if (f[n] == UNKNOWN) {
            f[n] = fib(n - 1) + fib(n - 2);
        }
        return f[n];
    }

    private int fibDriver(int n) {
        f = new int[n + 1];
        f[0] = 0;
        f[1] = 1;
        for (int i = 2; i < n; i++) {
            f[i] = UNKNOWN;
        }
        return fib(n);
    }
}
#+end_src

Explicit caching of the results of recursive calls provides most of the benefits of dynamic programming, including usually the same running time as the more elegant iterative solution. The added benefit of iterative solution is it leads to more space optimized solutions and can never run into stack overflow. Writing a iterative solution usually needs us to find a topological ordering of the search space DAG. In this fib example, if we find fib numbers from small to large, we will have all the values needed to calculate fib(n). Since we calculate from small to large numbers for fib(n), we only need to remember last two numbers i.e. fib(n-1) and fib(n-2). Thus we can avoid ~O(n)~ space complexity.
#+begin_src java
class Solution {
    private int fib(int n) {
        int back2 = 0, back1 = 1;
        int fib;
        if (n <= 0) {
            return back2;
        }
        if (n == 1) {
            return back1;
        }
        for (int i = 2; i <= n; i++) {
            fib = back2 + back1;
            back2 = back1;
            back1 = fib;
        }
        return fib;
    }
}
#+end_src
* When to use Dynamic Programming ?

* Steps to write a DP solution.

* Examples
